{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\loisn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\loisn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\loisn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import pickle\n",
    "import string \n",
    "import sys \n",
    "\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet','stopwords'])\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql_table('DS_messages', engine)\n",
    "X = df['message']\n",
    "y = df.drop(['message', 'genre', 'id', 'original'], axis=1)\n",
    "categories = y.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # normalize text and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    words = [w for w in tokens if w not in stop_words]\n",
    "    \n",
    "    # Reduce words to their stems\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = [stemmer.stem(w) for w in words]\n",
    "    \n",
    "    # Reduce words to their root form\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmed = [lemmatizer.lemmatize(w) for w in stemmed]\n",
    "    \n",
    "    return lemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bernourlli algorithm with custom parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(BernoulliNB()))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (1,2)),\n",
    "    'vect__max_features': (None, 5000,10000),\n",
    "    'tfidf__use_idf': (True, False)\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metrics(actual, predicted, categories):\n",
    "    \"\"\"Calculate evaluation metrics for ML model\n",
    "    \n",
    "    Args:\n",
    "    actual: array. Array containing actual labels.\n",
    "    predicted: array. Array containing predicted labels.\n",
    "    col_names: list of strings. List containing names for each of the predicted fields.\n",
    "       \n",
    "    Returns:\n",
    "    metrics_df: dataframe. Dataframe containing the precision, recall \n",
    "    and f1 score for a given set of actual and predicted labels.\n",
    "    \"\"\"\n",
    "    categories = y.columns.tolist()\n",
    "    metrics = []\n",
    "    \n",
    "    # Calculate evaluation metrics for each set of labels\n",
    "    for i in range(len(categories)):\n",
    "        accuracy = round(accuracy_score(actual[:, i], predicted[:, i]), 2)\n",
    "        recall = round(recall_score(actual[:, i], predicted[:, i], zero_division=0), 2)\n",
    "        precision = round(precision_score(actual[:, i], predicted[:, i], zero_division=0), 2)\n",
    "        f1 = round(f1_score(actual[:, i], predicted[:, i], zero_division=0), 2)\n",
    "        metrics.append([accuracy, precision, recall, f1])\n",
    "    \n",
    "    # Create dataframe containing metrics\n",
    "    metrics = np.array(metrics)\n",
    "    metrics_df = pd.DataFrame(data = metrics, index = categories, columns = ['accuracy', 'precision','recall', 'f1'])\n",
    "      \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('vect',\n",
       "                                        CountVectorizer(tokenizer=<function tokenize at 0x00000230A024FDC0>)),\n",
       "                                       ('tfidf', TfidfTransformer()),\n",
       "                                       ('clf',\n",
       "                                        MultiOutputClassifier(estimator=BernoulliNB()))]),\n",
       "             param_grid={'tfidf__use_idf': (True, False),\n",
       "                         'vect__max_df': (0.5, 0.75, 1.0),\n",
       "                         'vect__max_features': (None, 5000, 10000),\n",
       "                         'vect__ngram_range': ((1, 1), (1, 2))})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "cv.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        accuracy  precision  recall    f1\n",
      "related                     0.79       0.91    0.80  0.86\n",
      "request                     0.88       0.62    0.73  0.67\n",
      "offer                       0.99       0.00    0.00  0.00\n",
      "aid_related                 0.76       0.75    0.64  0.69\n",
      "medical_help                0.89       0.33    0.34  0.33\n",
      "medical_products            0.93       0.36    0.36  0.36\n",
      "search_and_rescue           0.96       0.16    0.12  0.14\n",
      "security                    0.97       0.06    0.02  0.03\n",
      "military                    0.95       0.31    0.33  0.32\n",
      "water                       0.94       0.54    0.53  0.53\n",
      "food                        0.92       0.65    0.65  0.65\n",
      "shelter                     0.91       0.50    0.46  0.48\n",
      "clothing                    0.98       0.27    0.14  0.19\n",
      "money                       0.97       0.20    0.11  0.14\n",
      "missing_people              0.98       0.09    0.04  0.06\n",
      "refugees                    0.95       0.19    0.18  0.18\n",
      "death                       0.94       0.35    0.37  0.36\n",
      "other_aid                   0.83       0.35    0.25  0.29\n",
      "infrastructure_related      0.89       0.22    0.26  0.24\n",
      "transport                   0.92       0.21    0.21  0.21\n",
      "buildings                   0.93       0.30    0.30  0.30\n",
      "electricity                 0.97       0.19    0.09  0.12\n",
      "tools                       0.99       0.13    0.07  0.09\n",
      "hospitals                   0.99       0.07    0.03  0.05\n",
      "shops                       0.99       0.10    0.07  0.08\n",
      "aid_centers                 0.98       0.17    0.07  0.10\n",
      "other_infrastructure        0.92       0.17    0.20  0.19\n",
      "weather_related             0.83       0.73    0.65  0.69\n",
      "floods                      0.90       0.41    0.51  0.46\n",
      "storm                       0.90       0.50    0.54  0.52\n",
      "fire                        0.99       0.12    0.06  0.08\n",
      "earthquake                  0.93       0.63    0.55  0.59\n",
      "cold                        0.97       0.21    0.16  0.18\n",
      "other_weather               0.90       0.18    0.22  0.20\n",
      "direct_report               0.81       0.50    0.72  0.59\n"
     ]
    }
   ],
   "source": [
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "# Show model evaluation result\n",
    "print(report_metrics(np.array(y_test), y_pred, categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli algorithm results better scores compared to RandomForestClassifier(), so I chose it as my final model for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best estimators on Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(max_df=0.5, max_features=10000,\n",
       "                                 ngram_range=(1, 2),\n",
       "                                 tokenizer=<function tokenize at 0x00000230A024FDC0>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultiOutputClassifier(estimator=BernoulliNB()))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DS_model.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib \n",
    "\n",
    "joblib.dump(cv, 'DS_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
